{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0a2f09-5a4a-4ef2-8d99-7f67df1b51e8",
   "metadata": {},
   "source": [
    "### [00] 실습을 시작하기 전에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5524c56f-0d56-4540-9e31-cf6e425eec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install newspaper3k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de11d82-8949-4333-a65a-984d826c5ef8",
   "metadata": {},
   "source": [
    "### [01] newspaper3k 라이브러리 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c9a03-c480-4694-8a9e-fb0b689f3ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "# ➊ 파싱할 뉴스 기사 URL 지정\n",
    "url = 'https://v.daum.net/v/20230211141701492'\n",
    "# ➋ 언어를 한국어로 설정하고 URL을 전달해 Article 클래스의 객체 생성\n",
    "article = Article(url, language='ko')\n",
    "# ➌ 지정된 웹페이지를 다운로드\n",
    "article.download()\n",
    "# ➍ 다운로드한 웹페이지를 분석하고 필요한 정보를 추출\n",
    "article.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc11538-a30b-4a31-ada6-0fd5683a9812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기사 제목 출력\n",
    "print('기사 제목 :',article.title)\n",
    "print('')\n",
    "# 기사 내용 출력\n",
    "print('기사 내용 :')\n",
    "print(article.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c960a-34cf-47a8-85e0-c8e5893b8a93",
   "metadata": {},
   "source": [
    "### [05] 뉴스 기사 크롤러 만들기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a279f-50e4-4495-aca2-0f985ac6abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT 카테고리에서 2024년 9월 12일 기준 전체 기사 목록의 2 페이지에 접근하는 URL을 동적으로 생성하는 코드\n",
    "page_num = 2 # ➊ 페이지 번호\n",
    "keyword = 'digital' # ➋ IT 카테고리\n",
    "date = 20240912 # ➌ 날짜\n",
    "\n",
    "\n",
    "# ➍ 변수를 넣어 URL 생성\n",
    "url = 'https://news.daum.net/breakingnews/'+keyword+'?page='+str(page_num)+'&regDate='+str(date)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994eacba-9700-4e73-aba9-60a79aad3775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성한 URL의 HTML 문서를 가져온 다음 text 속성으로 출력\n",
    "import requests\n",
    "\n",
    "news = requests.get(url)\n",
    "news.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee47b69-98ea-46a9-97d8-8d19c653c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# ➊ 페이지 수, 카테고리, 날짜를 전달받아 뉴스 기사의 URL을 추출하는 함수 make_urllist( ) 정의\n",
    "def make_urllist(page_num, keyword, date):\n",
    "  urllist= []\n",
    "  # ➋ 1페이지부터 page_num에 정해진 페이지만큼 for문으로 작업을 반복\n",
    "  for i in range(1, page_num + 1):\n",
    "    # ➌ 함수 호출 시 전달받은 인자를 조합해 다음 뉴스 페이지의 URL 생성\n",
    "    url = 'https://news.daum.net/breakingnews/'+keyword+'?page='+str(i)+'&regDate='+str(date)\n",
    "    # ➍ requets.get()으로 웹페이지의 HTML 문서 추출\n",
    "    news = requests.get(url)\n",
    "    # ➎ BeautifulSoup()으로 가져온 HTML 문서 파싱\n",
    "    soup = BeautifulSoup(news.text, 'html.parser')\n",
    "    # ➏ 뉴스 목록에 포함된 각 뉴스의 제목에 해당하는 a 태그를 가져와 리스트로 저장\n",
    "    news_list = soup.select('.list_allnews li div strong a')\n",
    "    # ➐ a의 href 속성값(url)만 추출해 urllist 리스트에 저장\n",
    "    for line in news_list:\n",
    "      urllist.append(line.get('href'))\n",
    "  # ➑ 함수 실행의 결과로 urllist 리스트 반환\n",
    "  return urllist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6d297-c8bb-4a36-903f-b5fde0d2de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_urllist() 호출\n",
    "url_list = make_urllist(2, 'economic', 20240912)\n",
    "print('뉴스 기사의 개수 :', len(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18733932-c06e-44c3-aba5-b82b4b3fb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 5개 URL 출력\n",
    "print(url_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ee4e05-fcc9-4054-bed7-78306dbf9e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 각 카테고리의 키워드와 실제 카테고리명을 매핑하는 딕셔너리 생성\n",
    "idx2word = {'society' : '사회', 'economic' : '경제', 'culture' : '문화', 'digital' : 'IT'}\n",
    "# ➊ 뉴스 기사 URL 리스트와 카테고리 키워드를 받아 기사 본문을 추출하는 함수 make_data() 생성\n",
    "def make_data(urllist, keyword):\n",
    "  text_list = []\n",
    "\n",
    "\n",
    "  # ➋ 주어진 URL 리스트를 반복하면서 각 뉴스 기사를 처리\n",
    "  for url in urllist:\n",
    "    article = Article(url, language='ko') # Article 객체 생성, 언어는 한국어로 설정\n",
    "    article.download() # 기사를 다운로드\n",
    "    article.parse() # 기사를 파싱\n",
    "    text_list.append(article.text) # 파싱된 텍스트를 리스트에 추가\n",
    "  # ➌ 'news' 열의 행마다 각 뉴스 기사 본문이 저장된 데이터프레임 생성\n",
    "  df = pd.DataFrame({'news': text_list})\n",
    "  # ➍ 기사 본문에서 줄바꿈 문자를 공백으로 치환\n",
    "  df['news'] = df['news'].str.replace('\\n',' ') # 가져온 뉴스 내용에서 \\n을 공백으로 바꿔주는 코드\n",
    "  # ➎ 데이터프레임에 category 열을 추가하고 딕셔너리 idx2word에서 해당 keyword에 대응하는 값 할당\n",
    "  df['category'] = idx2word[keyword]\n",
    "  # ➏ 완성된 데이터프레임 반환\n",
    "  return df\n",
    "\n",
    "\n",
    "# ➐ make_data 함수 호출, 특정 키워드에 해당하는 URL 리스트와 함께\n",
    "data = make_data(url_list, 'economic')\n",
    "data[:10] # 상위 10개 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6889767d-cce5-49d1-ab7b-5c02f7b30ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list의 첫 번째 요소를 출력\n",
    "print(url_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d45f7-1a6b-4d0d-83a5-e618a42c6486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리 키워드 리스트 생성\n",
    "keyword_list = ['society', 'economic', 'culture', 'digital']\n",
    "\n",
    "\n",
    "# ➊ 페이지 수와 키워드 리스트, 날짜를 인자로 받아 뉴스 기사를 추출하는 make_total_data()함수 생성\n",
    "def make_total_data(page_num, keyword_list, date):\n",
    "  # ➋ 초기 데이터프레임을 None으로 설정\n",
    "  df = None\n",
    "  # ➌ keyword_list의 요소를 순회하며 다음의 작업을 반복\n",
    "  for keyword in keyword_list:\n",
    "    # ➍ page_num과 date, 그리고 현재 keyword를 전달해 make_urllist()실행\n",
    "    url_list = make_urllist(page_num, keyword, date)\n",
    "    # ➎ url_list와 현재 keyword를 전달해 make_data() 실행\n",
    "    df_temp = make_data(url_list, keyword)\n",
    "    # ➏ 진행 상황을 확인하기 위한 문자열 출력\n",
    "    print(f'{keyword} 카테고리의 뉴스 기사 데이터 추출을 완료했습니다.')\n",
    "    # ➐ 데이터프레임 df에 값이 존재하면 df와 df_temp를 병합\n",
    "    if df is not None:\n",
    "      df = pd.concat([df, df_temp], ignore_index=True)\n",
    "    # ➑ df가 None이면 df_temp를 df로 할당\n",
    "    else:\n",
    "      df = df_temp\n",
    "\n",
    "\n",
    "  # ➒ 모든 카테고리에 대한 기사 수집이 완료되면 최종 df를 반환\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9b038-eee2-4c0b-9248-ab25a26a6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24년 9월 12일의 각 카테고리별 뉴스 기사를 2페이지까지 수집해 데이터프레임에 저장\n",
    "df = make_total_data(2, keyword_list, 20240912)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d69ef2-13e8-47a3-ba5b-e5f90f48e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('뉴스 기사의 개수 :', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6084e1-e286-4204-896c-7bec00773a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample( ) 함수로 데이터프레임의 데이터를 10개만 랜덤으로 뽑아서 출력\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720bb2d2-af88-4772-8021-aa877ec38eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스기사 크롤러 전체 코드\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ➊ 페이지 수, 카테고리, 날짜를 전달받아 뉴스 기사의 URL을 추출하는 함수 make_urllist( ) 정의\n",
    "def make_urllist(page_num, keyword, date):\n",
    "  urllist= []\n",
    "  # ➋ 1페이지부터 page_num에 정해진 페이지만큼 for문으로 작업을 반복\n",
    "  for i in range(1, page_num + 1):\n",
    "    # ➌ 함수 호출 시 전달받은 인자를 조합해 다음 뉴스 페이지의 URL 생성\n",
    "    url = 'https://news.daum.net/breakingnews/'+keyword+'?page='+str(i)+'&regDate='+str(date)\n",
    "    # ➍ requets.get()으로 웹페이지의 HTML 문서 추출\n",
    "    news = requests.get(url)\n",
    "    # ➎ BeautifulSoup()으로 가져온 HTML 문서 파싱\n",
    "    soup = BeautifulSoup(news.text, 'html.parser')\n",
    "    # ➏ 뉴스 목록에 포함된 각 뉴스의 제목에 해당하는 a 태그를 가져와 리스트로 저장\n",
    "    news_list = soup.select('.list_allnews li div strong a')\n",
    "    # ➐ a의 href 속성값(url)만 추출해 urllist 리스트에 저장\n",
    "    for line in news_list:\n",
    "      urllist.append(line.get('href'))\n",
    "  # ➑ 함수 실행의 결과로 urllist 리스트 반환\n",
    "  return urllist\n",
    "\n",
    "# 각 카테고리의 키워드와 실제 카테고리명을 매핑하는 딕셔너리 생성\n",
    "idx2word = {'society' : '사회', 'economic' : '경제', 'culture' : '문화', 'digital' : 'IT'}\n",
    "# ➊ 뉴스 기사 URL 리스트와 카테고리 키워드를 받아 기사 본문을 추출하는 함수 make_data() 생성\n",
    "def make_data(urllist, keyword):\n",
    "  text_list = []\n",
    "\n",
    "\n",
    "  # ➋ 주어진 URL 리스트를 반복하면서 각 뉴스 기사를 처리\n",
    "  for url in urllist:\n",
    "    article = Article(url, language='ko') # Article 객체 생성, 언어는 한국어로 설정\n",
    "    article.download() # 기사를 다운로드\n",
    "    article.parse() # 기사를 파싱\n",
    "    text_list.append(article.text) # 파싱된 텍스트를 리스트에 추가\n",
    "  # ➌ 'news' 열의 행마다 각 뉴스 기사 본문이 저장된 데이터프레임 생성\n",
    "  df = pd.DataFrame({'news': text_list})\n",
    "  # ➍ 기사 본문에서 줄바꿈 문자를 공백으로 치환\n",
    "  df['news'] = df['news'].str.replace('\\n',' ') # 가져온 뉴스 내용에서 \\n을 공백으로 바꿔주는 코드\n",
    "  # ➎ 데이터프레임에 category 열을 추가하고 딕셔너리 idx2word에서 해당 keyword에 대응하는 값 할당\n",
    "  df['category'] = idx2word[keyword]\n",
    "  # ➏ 완성된 데이터프레임 반환\n",
    "  return df\n",
    "\n",
    "\n",
    "# ➐ make_data 함수 호출, 특정 키워드에 해당하는 URL 리스트와 함께\n",
    "data = make_data(url_list, 'economic')\n",
    "data[:10] # 상위 10개 출력\n",
    "\n",
    "\n",
    "# 카테고리 키워드 리스트 생성\n",
    "keyword_list = ['society', 'economic', 'culture', 'digital']\n",
    "\n",
    "\n",
    "# ➊ 페이지 수와 키워드 리스트, 날짜를 인자로 받아 뉴스 기사를 추출하는 make_total_data()함수 생성\n",
    "def make_total_data(page_num, keyword_list, date):\n",
    "  # ➋ 초기 데이터프레임을 None으로 설정\n",
    "  df = None\n",
    "  # ➌ keyword_list의 요소를 순회하며 다음의 작업을 반복\n",
    "  for keyword in keyword_list:\n",
    "    # ➍ page_num과 date, 그리고 현재 keyword를 전달해 make_urllist()실행\n",
    "    url_list = make_urllist(page_num, keyword, date)\n",
    "    # ➎ url_list와 현재 keyword를 전달해 make_data() 실행\n",
    "    df_temp = make_data(url_list, keyword)\n",
    "    # ➏ 진행 상황을 확인하기 위한 문자열 출력\n",
    "    print(f'{keyword} 카테고리의 뉴스 기사 데이터 추출을 완료했습니다.')\n",
    "    # ➐ 데이터프레임 df에 값이 존재하면 df와 df_temp를 병합\n",
    "    if df is not None:\n",
    "      df = pd.concat([df, df_temp], ignore_index=True)\n",
    "    # ➑ df가 None이면 df_temp를 df로 할당\n",
    "    else:\n",
    "      df = df_temp\n",
    "\n",
    "\n",
    "  # ➒ 모든 카테고리에 대한 기사 수집이 완료되면 최종 df를 반환\n",
    "  return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
