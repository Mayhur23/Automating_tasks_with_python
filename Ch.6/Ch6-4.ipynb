{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cacf1532-f996-48f1-b0eb-544a52f61f62",
   "metadata": {},
   "source": [
    "### [00] 실습을 시작하기 전에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c94ab-4967-4b3a-a2c5-88f72cfba632",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install newspaper3k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f6730-a691-44ed-bc76-07d8c99c2b0d",
   "metadata": {},
   "source": [
    "### [01] newspaper3k 라이브러리 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a572d3dc-b4be-44bc-aa9c-ba80529ac1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "# ➊ 파싱할 뉴스 기사 URL 지정\n",
    "url = 'https://news.nate.com/view/20230211n06158?mid=n0601'\n",
    "\n",
    "# ➋ 언어를 한국어로 설정하고 URL을 전달해 Article 클래스의 객체 생성\n",
    "article = Article(url, language='ko')\n",
    "\n",
    "# ➌ 지정된 웹페이지를 다운로드\n",
    "article.download()\n",
    "\n",
    "# ➍ 다운로드한 웹페이지를 분석하고 필요한 정보를 추출\n",
    "article.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cee126-795f-424b-a99d-6427ad6c813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기사 제목 출력\n",
    "print('기사 제목 :',article.title)\n",
    "print('')\n",
    "\n",
    "# 기사 내용 출력\n",
    "print('기사 내용 :')\n",
    "print(article.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9019f990-c61c-4ca8-b445-03b9e0f58932",
   "metadata": {},
   "source": [
    "### [05] 뉴스 기사 크롤러 만들기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6510eb-2117-400e-9012-6eef36b603e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_num = 2 # ➊ 페이지 번호\n",
    "category1 = 'its99' # ➋ IT/과학 일반 카테고리 코드1\n",
    "category2 = '0607' # ➋ IT/과학 일반 카테고리 코드2\n",
    "date = 20240912 # ➌ 날짜\n",
    "\n",
    "# ➍ 변수를 넣어 URL 생성\n",
    "url = 'https://news.nate.com/subsection?cate='+category1+'&mid=n'+category2+'&type=c&date='+str(date)+'?page='+str(page_num)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677e2eb2-4a00-49a5-a080-63d513bc0436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "news = requests.get(url)\n",
    "news.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375bd2a2-b9be-4a3a-8277-127f84d8d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# ➊ 페이지 수, 카테고리, 날짜를 전달받아 뉴스 기사의 URL을 추출하는 함수 make_urllist( ) 정의\n",
    "def make_urllist(page_num, category1, category2, date):\n",
    "  urllist= []\n",
    "    \n",
    "  # ➋ 1페이지부터 page_num에 정해진 페이지만큼 for문으로 작업을 반복\n",
    "  for i in range(1, page_num + 1):\n",
    "    # ➌ 함수 호출 시 전달받은 인자를 조합해 다음 뉴스 페이지의 URL 생성\n",
    "    url = 'https://news.nate.com/subsection?cate='+category1+'&mid=n'+category2+'&type=c&date='+str(date)+'?page='+str(i)\n",
    "    # ➍ requets.get()으로 웹페이지의 HTML 문서 추출\n",
    "    news = requests.get(url)\n",
    "    # ➎ BeautifulSoup()으로 가져온 HTML 문서 파싱\n",
    "    soup = BeautifulSoup(news.text, 'html.parser')\n",
    "    # ➏ 뉴스 목록에 포함된 각 뉴스의 제목에 해당하는 a 태그를 가져와 리스트로 저장\n",
    "    news_list = soup.select('.postSubjectContent .mduSubjectList .mlt01 a')\n",
    "      \n",
    "    # ➐ a의 href 속성값(url)만 추출해 urllist 리스트에 저장\n",
    "    for line in news_list:\n",
    "      urllist.append(line.get('href'))\n",
    "        \n",
    "  # ➑ 함수 실행의 결과로 urllist 리스트 반환\n",
    "  return urllist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f928606-8e61-4c39-88a7-dae18498abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = make_urllist(2, 'its99', '0607', 20240912)\n",
    "print('뉴스 기사의 개수 :', len(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df0724-ce3d-4fd5-ac31-0cec9d1a9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(url_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03815842-2a13-4691-8216-0680d29af74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# IT/과학 카테고리의 하위 카테고리 키워드와 실제 카테고리명을 매핑하는 딕셔너리 생성\n",
    "idx2word = {'0602' : '과학', '0604' : '컴퓨터/디지털', '0605' : '뉴미디어/통신', '0607' : 'IT/과학 일반'}\n",
    "\n",
    "# ➊ 뉴스 기사 URL 리스트와 카테고리 키워드를 받아 기사 본문을 추출하는 함수 make_data() 생성\n",
    "def make_data(urllist, category1, category2):\n",
    "  text_list = []\n",
    "\n",
    "\n",
    "  # ➋ 주어진 URL 리스트를 반복하면서 각 뉴스 기사를 처리\n",
    "  for url in urllist:\n",
    "    url = 'https:' + url\n",
    "    article = Article(url, language='ko') # Article 객체 생성, 언어는 한국어로 설정\n",
    "    article.download() # 기사를 다운로드\n",
    "    article.parse() # 기사를 파싱\n",
    "    text_list.append(article.text) # 파싱된 텍스트를 리스트에 추가\n",
    "  # ➌ 'news' 열의 행마다 각 뉴스 기사 본문이 저장된 데이터프레임 생성\n",
    "  df = pd.DataFrame({'news': text_list})\n",
    "  # ➍ 기사 본문에서 줄바꿈 문자를 공백으로 치환\n",
    "  df['news'] = df['news'].str.replace('\\n',' ') # 가져온 뉴스 내용에서 \\n을 공백으로 바꿔주는 코드\n",
    "  # ➎ 데이터프레임에 category 열을 추가하고 딕셔너리 idx2word에서 해당 keyword에 대응하는 값 할당\n",
    "  df['category'] = idx2word[category2]\n",
    "  # ➏ 완성된 데이터프레임 반환\n",
    "  return df\n",
    "\n",
    "\n",
    "# ➐ make_data 함수 호출, 특정 키워드에 해당하는 URL 리스트와 함께\n",
    "data = make_data(url_list, 'its99', '0607')\n",
    "data[:10] # 상위 10개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8eb46-b85c-4ff2-9039-131bb227c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list의 첫 번째 요소를 출력\n",
    "print('https:' + url_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ba822-c807-4d3b-afad-af920d09f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리 코드 리스트 생성\n",
    "category_list = [('its01','0602'), ('its03','0604'), ('its04','0605'), ('its99','0607')]\n",
    "\n",
    "\n",
    "# ➊ 페이지 수와 키워드 리스트, 날짜를 인자로 받아 뉴스 기사를 추출하는 make_total_data()함수 생성\n",
    "def make_total_data(page_num, keyword_list, date):\n",
    "  # ➋ 초기 데이터프레임을 None으로 설정\n",
    "  df = None\n",
    "  # ➌ keyword_list의 요소를 순회하며 다음의 작업을 반복\n",
    "  for cate1, cate2 in category_list:\n",
    "    # ➍ page_num과 date, 그리고 현재 카테고리 코드(cate1, cate2)를 전달해 make_urllist()실행\n",
    "    url_list = make_urllist(page_num, cate1, cate2, date)\n",
    "    # ➎ url_list와 현재 카테고리 코드(cate1, cate2)를 전달해 make_data() 실행\n",
    "    df_temp = make_data(url_list, cate1, cate2)\n",
    "    # ➏ 진행 상황을 확인하기 위한 문자열 출력\n",
    "    print(f'{cate1} 카테고리의 뉴스 기사 데이터 추출을 완료했습니다.')\n",
    "    # ➐ 데이터프레임 df에 값이 존재하면 df와 df_temp를 병합\n",
    "    if df is not None:\n",
    "      df = pd.concat([df, df_temp], ignore_index=True)\n",
    "    # ➑ df가 None이면 df_temp를 df로 할당\n",
    "    else:\n",
    "      df = df_temp\n",
    "\n",
    "\n",
    "  # ➒ 모든 카테고리에 대한 기사 수집이 완료되면 최종 df를 반환\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a194e020-db15-4159-8f53-eae495d2a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24년 9월 12일의 각 카테고리별 뉴스 기사를 2페이지까지 수집해 데이터프레임에 저장\n",
    "df = make_total_data(2, category_list, 20240912)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c871b-ac32-4eda-befa-5686b01f0a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('뉴스 기사의 개수 :', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73c9cf-8a36-4b57-91aa-85b6dc3e2ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample( ) 함수로 데이터프레임의 데이터를 10개만 랜덤으로 뽑아서 출력\n",
    "df.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
